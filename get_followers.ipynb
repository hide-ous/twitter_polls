{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from data_utils import get_followers_from_lists, DATA_PATH, get_scaraped_profiles, get_retweeters_repliers, get_author_ids, get_follow_from_lists\n",
    "from twitter_utils import get_client, get_profiles\n",
    "\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "client = get_client()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "ids = get_followers_from_lists()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "12439182"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12439182 users were already scraped\n",
      "0 users left to scrape\n"
     ]
    }
   ],
   "source": [
    "out_path = os.path.join(DATA_PATH, 'followers_rehydrated.jsonl')\n",
    "err_path = os.path.join(DATA_PATH, 'followers_missing.jsonl')\n",
    "already_scraped = get_scaraped_profiles(out_path, err_path)\n",
    "print(f\"{len(already_scraped)} users were already scraped\")\n",
    "\n",
    "ids_ = [id_ for id_ in ids if id_ not in already_scraped]\n",
    "print(f\"{len(ids_)} users left to scrape\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|██████████| 6/6 [00:02<00:00,  2.58it/s]\n"
     ]
    }
   ],
   "source": [
    "errors = get_profiles(ids=ids_, client=client, out_path=out_path, error_path=err_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(errors))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|██████████| 2/2 [00:00<00:00,  3.16it/s]\n"
     ]
    }
   ],
   "source": [
    "other_errors = get_profiles(ids=errors, client=client, out_path=out_path, error_path=err_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "8155"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repliers = get_retweeters_repliers()\n",
    "len(repliers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8157 users were already scraped\n",
      "8155 users left to scrape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|██████████| 82/82 [00:31<00:00,  2.61it/s]\n"
     ]
    }
   ],
   "source": [
    "out_path = os.path.join(DATA_PATH, 'repliers_rehydrated.jsonl')\n",
    "err_path = os.path.join(DATA_PATH, 'repliers_missing.jsonl')\n",
    "\n",
    "already_scraped = get_scaraped_profiles(out_path, err_path)\n",
    "print(f\"{len(already_scraped)} users were already scraped\")\n",
    "\n",
    "repliers = [id_ for id_ in repliers if id_ not in already_scraped]\n",
    "print(f\"{len(repliers)} users left to scrape\")\n",
    "\n",
    "follower_errors = get_profiles(ids=repliers, client=client, out_path=out_path, error_path=err_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(follower_errors)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "author_ids = set(get_author_ids(2016))\n",
    "author_ids.update(get_author_ids(2020))\n",
    "author_ids = list(sorted(author_ids))\n",
    "print(len(author_ids))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|██████████| 11/11 [00:04<00:00,  2.51it/s]\n"
     ]
    }
   ],
   "source": [
    "out_path = os.path.join(DATA_PATH, 'authors_rehydrated.jsonl')\n",
    "err_path = os.path.join(DATA_PATH, 'author_profiles_missing.jsonl')\n",
    "\n",
    "already_scraped = get_scaraped_profiles(out_path, err_path)\n",
    "print(f\"{len(already_scraped)} users were already scraped\")\n",
    "\n",
    "author_ids = [id_ for id_ in author_ids if id_ not in already_scraped]\n",
    "print(f\"{len(ids_)} users left to scrape\")\n",
    "\n",
    "author_errors = get_profiles(ids=author_ids, client=client, out_path=out_path, error_path=err_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(author_errors)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'follower_lists_new.jsonl'\n",
    "os.path.join(DATA_PATH, )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 users were already scraped\n",
      "3759136 users left to scrape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:   1%|          | 300/37592 [01:57<3:56:43,  2.63it/s]Rate limit exceeded. Sleeping for 784 seconds.\n"
     ]
    }
   ],
   "source": [
    "author_ids = get_follow_from_lists()\n",
    "out_path = os.path.join(DATA_PATH, 'new_followers_rehydrated.jsonl')\n",
    "err_path = os.path.join(DATA_PATH, 'new_followers_missing.jsonl')\n",
    "\n",
    "already_scraped = get_scaraped_profiles(out_path, err_path)\n",
    "print(f\"{len(already_scraped)} users were already scraped\")\n",
    "\n",
    "author_ids = [id_ for id_ in author_ids if id_ not in already_scraped]\n",
    "print(f\"{len(author_ids)} users left to scrape\")\n",
    "\n",
    "follower_errors = get_profiles(ids=author_ids, client=client, out_path=out_path, error_path=err_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "author_ids = get_follow_from_lists('followee_lists_of_new_retweeters_repliers.jsonl')\n",
    "out_path = os.path.join(DATA_PATH, 'new_followees_rehydrated.jsonl')\n",
    "err_path = os.path.join(DATA_PATH, 'new_followees_missing.jsonl')\n",
    "\n",
    "already_scraped = get_scaraped_profiles(out_path, err_path)\n",
    "print(f\"{len(already_scraped)} users were already scraped\")\n",
    "\n",
    "author_ids = [id_ for id_ in author_ids if id_ not in already_scraped]\n",
    "print(f\"{len(author_ids)} users left to scrape\")\n",
    "\n",
    "followee_errors = get_profiles(ids=author_ids, client=client, out_path=out_path, error_path=err_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
